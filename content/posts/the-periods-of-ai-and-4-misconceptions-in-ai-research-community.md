---
title: "The periods of AI and 4 misconceptions in AI Research community "
date: 2021-05-18T16:23:55.376Z
published: true
tags:
  - ai
  - deep-learning
  - research
description: >-
  In recent years, AI has appeared a lot in the media. Is it really miraculous
  as people say, or just a hype?


  Let's get this problem enlightened a bit through exploring the sufferings of AI formation in the paper “Why AI is harder than we think”
---


- - -

Until now, AI has gone up and down for many times. People call its periods as the Springs and Winters of AI. In the 1960s and early 1970s, there had been many over-optimistic predictions about AI but they soon backed out for AI winter. There was an upsurge in eagerness in the early 1980s with several new initiatives. In the latter parts of the 1980s, with the problems of brittleness and lack of generality, hopes of AI had been all halted once again. The neural network approaches of this period cannot be extended to complex problems. Afterwards, statistical machine learning started its spectacular rise in the 1990s and 2000s. However, it was not until 2010 that deep learning rose to monumental status. All of a sudden, the term ‘AI’ became ubiquitous.

The author also points out 4 fallacies that have long (and still) existed in the AI research community:

* **“Narrow intelligence is on a continuum with general intelligence”**: To understand this fallacy, you must first know what a continuum is. It is a sequence of elements in which 2 side-by-side elements are almost the same but the 2 extremes are very different. Imagine that general AI is at one last end of the continuum and a progress in an AI task is at somewhere else of the continuum. Describing advances on a particular AI task  as “a step toward general AI” like some papers today makes no sense because no matter what the size of a contribution, they all can be seen as “on a continuum with general intelligence”.
* **“Easy things are easy and hard things are hard”**: This is about a paradox. Things that humans do easily is a hard task for AI (like avoiding running into other people when walking), things that humans have problems doing, in contrast AI can solve very well (like playing chess). One explanation may be those simple tasks may require subconscious parts of the brain which current AIs don’t have. So, maybe researchers should think in an opposite way that is “Easy things are hard and hard things are easy” when developing a new idea of AI.
* **“The lure of wishful mnemonics”**: This fallacy is to reflect the way researchers use words in their program/procedure description. They named their program‘s operations with words such as “UNDERSTAND” or “THOUGHT”,... “Wishful mnemonics” is what Drew McDermott called these aspirational terms \[2]. Using such human words can unintentionally cause a risk of misunderstandings to people that AI has achieved common sense as humans even though it hasn’t. Currently, work on AI is still filled with such wishful mnemonics.
* **“Intelligence is all in the brain”**: Simulating AI by only the human brain part is not sufficient, there must be some connections with the body. Nevertheless, researchers have just focused on increasing the computational power of resources for AI to reach the human-level ability. According to a number of cognitive scientists:  “Our thoughts are grounded, or inextricably associated with perception, action, and emotion, and that our brain and body work together to have cognition” \[1].

- - -

All in all, it is just a matter of time until AI replaces humans. No one knows when it will happen, maybe next year, next century, next millennium or even more than that. Whether it happens or not, only one thing we can perceive for now is that AI will definitely be able to support us do most of the things in the near future.

- - -

## References:

\[1] Melanie Mitchell, [Why AI is harder than we think?](https://arxiv.org/pdf/2104.12871.pdf), arXiv, 2021.

\[2] Noel Sharkey and Lucy Suchman, [Wishful Mnemonics and Autonomous Killing Machines](https://eprints.lancs.ac.uk/id/eprint/65657/1/Sharkey_Suchman_AISBQ_136.pdf), 2013.